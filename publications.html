<!DOCTYPE html>
<html lang="zxx">

<head>
    <meta charset="utf-8">
    <title>COG-MHEAR Publications</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Towards cognitively-inspired 5G-IoT enabled, multi-modal Hearing Aids">
    <meta name="author" content="">

    <!--[if lt IE 9]>
	<script src="js/html5shiv.js"></script>
	<![endif]-->

    <!-- CSS Files
    ================================================== -->
    <link rel="stylesheet" href="css/bootstrap.min.css" type="text/css">
	<link rel="stylesheet" href="css/bootstrap-grid.min.css" type="text/css">
	<link rel="stylesheet" href="css/bootstrap-reboot.min.css" type="text/css">
    <link rel="stylesheet" href="css/animate.css" type="text/css">
    <link rel="stylesheet" href="css/owl.carousel.css" type="text/css">
    <link rel="stylesheet" href="css/owl.theme.css" type="text/css">
    <link rel="stylesheet" href="css/owl.transitions.css" type="text/css">
    <link rel="stylesheet" href="css/magnific-popup.css" type="text/css">
    <link rel="stylesheet" href="css/jquery.countdown.css" type="text/css">
    <link rel="stylesheet" href="css/style.css" type="text/css">
    <link rel="stylesheet" href="css/colors/red.css" type="text/css">
    <link rel="stylesheet" href="css/custom.css" type="text/css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7JWKH69N61"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7JWKH69N61');
</script>
</head>

<body>


    <div id="topbar" class="text-white topbar-noborder">
        <div class="container">
            <div class="topbar-right sm-hide">
                        <span class="topbar-widget tb-social">
                            <a href="mailto:cogmhear@napier.ac.uk" target="_blank"><i class="fa fa-envelope-square"></i></a>
                            <a href="https://twitter.com/cogmhear" target="_blank"><i class="fa fa-twitter-square"></i></a>
                            <a href="https://www.linkedin.com/in/cog-mhear-research-programme-55a016223/" target="_blank"><i class="fa fa-linkedin-square"></i></a>
                        </span>
            </div>
            <div class="clearfix"></div>
        </div>
    </div>
    <div id="wrapper">

        <div class="page-overlay">
            <div class="preloader-wrap">
                <div class="spinner">
                    <div class="bounce1"></div>
                    <div class="bounce2"></div>
                    <div class="bounce3"></div>
                </div>
            </div>
        </div>

        <!-- header begin -->
        <header>

            <div class="container">
                <div class="row">
                    <div class="col-md-12">
                        <!-- logo begin -->
                        <div id="logo">
                            <a href="index.html">
                                <img class="logo" src="img/logo-light.png" alt="" style="padding-top:10px;max-height: 50px;">
                                <img class="logo-2" src="img/logo-dark.png" alt="" style="padding-top:30px;max-height: 70px;">
                            </a>
                        </div>
                        <!-- logo close -->

                        <!-- small button begin -->
                        <span id="menu-btn"></span>
                        <!-- small button close -->

                        <!-- mainmenu begin -->
                        <nav>
                            <ul id="mainmenu">
                                <li><a href="index.html#about">About</a></li>
                                <li><a href="research.html">Research</a></li>
                                <li><a href="https://blog.cogmhear.org">Blog</a></li>
                                <!--                                <li><a href="workwithus.html">Work with us</a></li>-->
                                <li><a href="publications.html">Resources</a></li>
                                <li><a href="team.html">Team</a></li>
                                <li><a href="partners.html">Partners</a></li>
                                <li><a href="news.html">News</a></li>
                                <li><a href="contact.html">Get Involved</a></li>
                            </ul>
                        </nav>
                        <!-- mainmenu close -->

                    </div>

                </div>
            </div>
        </header>
        <!-- header close -->

        <!-- content begin -->
        <div id="content" class="no-bottom no-top">
            <div id="top"></div>

            <!-- section begin -->
            <section id="subheader" class="text-light" data-bgimage="url(img/publications.jpg)" data-stellar-background-ratio=".2">
                <div class="overlay-bg t50">

                    <div class="container">
                        <div class="row">
                            <div class="col-md-12">
                                <h2>Publications and Project Resources</h2>
                            </div>
                        </div>
                    </div>

                </div>

            </section>
            <!-- section close -->
            <section id="section-side" class="side-bg no-padding">

                <div class="container">
                    <div class="row">
                        <div class="inner-padding">
                            <div class="col-md-12 wow fadeIn">
                                <h3 class="mb20">5G IoT enabled Hearing Aid Demo</h3>
<!--                                                                <img style="max-width: 100%;" src="img/papers/1.jpg">-->
                                <p class="text-justify">
                                    In the demo, the left-hand side system act as hearing aid (HA) device, and the right-hand side system acts as a cloud for running speech enhancement algorithms. The channel between the hearing aid device to the access point is termed as an uplink channel and the channel between the access point and the HA device is termed as a downlink channel. Due to the real time nature of the information received at HA devices, the uplink channel supports varying data-rate and hence long-term evolution (LTE) based modified frame structure (The 5G-NR frame structure with 15 kHz subcarrier spacing is same as LTE frame structure) is developed for uplink data transmission. It supports 1.4 MHz and 3 MHz bandwidth with different modulation and code-rate for error correction codes. However, the cloud access point only transmits audio information to the HA device and hence supports a fixed data rate. Thus, again an LTE-based modified frame structure with 1.4 MHz bandwidth is developed for real-time speech enhancement.
                                </p>
                                <iframe src="https://player.vimeo.com/video/675527544?h=5a1bf84bf8" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
                                <div class="spacer-half"></div>
                            </div>
                            <div class="clearfix"></div>
                        </div>
                        <div class="inner-padding">
                            <div class="col-md-12 wow fadeIn">
                                <h3 class="mb20">5G-enabled contactless multi-user presence and activity detection for independent assisted living</h3>
<!--                                <img style="max-width: 100%;" src="img/papers/1.jpg">-->
                                <p class="text-justify">Abstract: Wireless sensing is the state-of-the-art technique for next generation health activity monitoring. Smart homes and healthcare centres have a demand for multi-subject health activity monitoring to cater for future requirements. 5G-sensing coupled with deep learning models has enabled smart health monitoring systems, which have the potential to classify multiple activities based on variations in channel state information (CSI) of wireless signals. Proposed is the first 5G-enabled system operating at 3.75 GHz for multi-subject, in-home health activity monitoring, to the best of the authorsâ€™ knowledge. Classified are activities of daily life performed by up to 4 subjects, in 16 categories. The proposed system combines subject count and activities performed in different classes together, resulting in simultaneous identification of occupancy count and activities performed. The CSI amplitudes obtained from 51 subcarriers of the wireless signal are processed and combined to capture variations due to simultaneous multi-subject movements. A deep learning convolutional neural network is engineered and trained on the CSI data to differentiate multi-subject activities. The proposed system provides a high average accuracy of 91.25% for single subject movements and an overall high multi-class accuracy of 83% for 4 subjects and 16 classification categories. The proposed system can potentially fulfill the needs of future in-home health activity monitoring and is a viable alternative for monitoring public health and well being.
                                </p>
                                <div class="spacer-half"></div>
                                <a href="http://eprints.gla.ac.uk/249524/1/249524.pdf" class="btn-custom scroll-to">PDF</a><a href="https://researchdata.gla.ac.uk/1151/" class="btn-custom" style="margin-left: 10px;">Data</a>
                            </div>
                            <div class="clearfix"></div>
                        </div>

                        <div class="inner-padding">
                            <div class="col-md-12 wow fadeIn">
                                <h3 class="mb20">Towards Robust Real-time Audio-Visual Speech Enhancement</h3>
                                <img style="max-width: 100%;" src="img/papers/3.jpg">
                                <p class="text-justify">Abstract: The human brain contextually exploits heterogeneous sensory information to efficiently perform cognitive tasks including vision and hearing. For example, during the cocktail party situation, the human auditory cortex contextually integrates audio-visual (AV) cues in order to better perceive speech. Recent studies have shown that AV speech enhancement (SE) models can significantly improve speech quality and intelligibility in very low signal to noise ratio (SNR) environments as compared to audio-only SE models. However, despite significant research in the area of AV SE, development of real-time processing models with low latency remains a formidable technical challenge. In this paper, we present a novel framework for low latency speaker-independent AV SE that can generalise on a range of visual and acoustic noises. In particular, a generative adversarial networks (GAN) is proposed to address the practical issue of visual imperfections in AV SE. In addition, we propose a deep neural network based real-time AV SE model that takes into account the cleaned visual speech output from GAN to deliver more robust SE. The proposed framework is evaluated on synthetic and real noisy AV corpora using objective speech quality and intelligibility metrics and subjective listing tests. Comparative simulation results show that our real time AV SE framework outperforms state-of-the-art SE approaches, including recent DNN based SE models.
                                </p>
                                <div class="spacer-half"></div>
                                <a href="https://arxiv.org/pdf/2112.09060v1.pdf" class="btn-custom scroll-to" target="_blank">PDF</a>
                            </div>
                            <div class="clearfix"></div>
                        </div>

                        <div class="inner-padding">
                            <div class="col-md-12 wow fadeIn">
                                <h3 class="mb20">Towards Intelligibility-Oriented Audio-Visual Speech Enhancement</h3>
                                <img style="max-width: 100%;" src="img/papers/2.jpg">
                                <p class="text-justify">Abstract: Existing deep learning (DL) based speech enhancement approaches are generally optimised to minimise the distance between clean and enhanced speech features. These often result in improved speech quality however they suffer from a lack of generalisation and may not deliver the required speech intelligibility in real noisy situations. In an attempt to address these challenges, researchers have explored intelligibility-oriented (I-O) loss functions and integration of audio-visual (AV) information for more robust speech enhancement (SE). In this paper, we introduce DL based I-O SE algorithms exploiting AV information, which is a novel and previously unexplored research direction. Specifically, we present a fully convolutional AV SE model that uses a modified short-time objective intelligibility (STOI) metric as a training cost function. To the best of our knowledge, this is the first work that exploits the integration of AV modalities with an I-O based loss function for SE. Comparative experimental results demonstrate that our proposed I-O AV SE framework outperforms audio-only (AO) and AV models trained with conventional distance-based loss functions, in terms of standard objective evaluation measures when dealing with unseen speakers and noises.
                                </p>
                                <div class="spacer-half"></div>
                                <a href="https://claritychallenge.github.io/clarity2021-workshop/papers/Clarity_2021_CEC1_paper_final_hussain.pdf" class="btn-custom scroll-to" target="_blank">PDF</a><a href="https://github.com/cogmhear/Intelligibility-Oriented-Audio-Visual-Speech-Enhancement" class="btn-custom scroll-to" style="margin-left: 10px;">GitHub</a>
                            </div>
                            <div class="clearfix"></div>
                        </div>

                        <div class="inner-padding">
                            <div class="col-md-12 wow fadeIn">
                                <h3 class="mb20">CochleaNet: A robust language-independent audio-visual model for real-time speech enhancement</h3>
                                <img style="max-width: 100%;" src="img/papers/1.jpg">
                                <p class="text-justify">Abstract: Noisy situations cause huge problems for the hearing-impaired, as hearing aids often make speech more audible but do not always restore intelligibility. In noisy settings, humans routinely exploit the audio-visual (AV) nature of speech to selectively suppress background noise and focus on the target speaker. In this paper, we present a novel language-, noise- and speaker-independent AV deep neural network (DNN) architecture, termed CochleaNet, for causal or real-time speech enhancement (SE). The model jointly exploits noisy acoustic cues and noise robust visual cues to focus on the desired speaker and improve speech intelligibility. The proposed SE framework is evaluated using a first of its kind AV binaural speech corpus, ASPIRE, recorded in real noisy environments, including cafeteria and restaurant settings. We demonstrate superior performance of our approach in terms of both objective measures and subjective listening tests, over state-of-the-art SE approaches, including recent DNN based SE models. In addition, our work challenges a popular belief that scarcity of a multi-lingual, large vocabulary AV corpus and a wide variety of noises is a major bottleneck to build robust language, speaker and noise-independent SE systems. We show that a model trained on a synthetic mixture of the benchmark GRID corpus (with 33 speakers and a small English vocabulary) and CHiME 3 noises (comprising bus, pedestrian, cafeteria, and street noises) can generalise well, not only on large vocabulary corpora with a wide variety of speakers and noises, but also on completely unrelated languages such as Mandarin.
                                </p>
                                <div class="spacer-half"></div>
                                <a href="https://arxiv.org/pdf/1909.10407.pdf" class="btn-custom scroll-to">PDF</a><a href="https://doi.org/10.1016/j.inffus.2020.04.001" class="btn-custom scroll-to" style="margin-left: 10px;">DOI</a><a href="https://zenodo.org/record/4585619" class="btn-custom" style="margin-left: 10px;">ASPIRE Corpus</a>
                            </div>
                            <div class="clearfix"></div>
                        </div>

                        <div class="inner-padding">
                            <div class="col-md-12 wow fadeIn">
                                <h3 class="mb20">Contextual deep learning-based audio-visual switching for speech enhancement in real-world environments</h3>
                                <img style="max-width: 100%;" src="img/papers/0.jpg">
                                <p class="text-justify">Abstract: Human speech processing is inherently multi-modal, where visual cues (e.g. lip movements) can help better understand speech in noise. Our recent work [1] has shown that lip-reading driven, audio-visual (AV) speech enhancement can significantly outperform benchmark audio-only approaches at low signal-to-noise ratios (SNRs). However, consistent with our cognitive hypothesis, visual cues were found to be relatively less effective for speech enhancement at high SNRs, or low levels of background noise, where audio-only (A-only) cues worked adequately. Therefore, a more cognitively-inspired, context-aware AV approach is required, that contextually utilises both visual and noisy audio features, and thus more effectively accounts for different noisy conditions. In this paper, we introduce a novel context-aware AV speech enhancement framework that contextually exploits AV cues with respect to different operating conditions, in order to estimate clean audio, without requiring any prior SNR estimation. In particular, an AV switching module is developed by integrating a convolutional neural network (CNN) and long-short-term memory (LSTM) network, that learns to contextually switch between visualonly (V-only), A-only and both AV cues at low, high and moderate SNR levels, respectively. For testing, the estimated clean audio features are utilised using an innovative, enhanced visually-derived Wiener filter (EVWF) for noisy speech filtering. The context-aware AV speech enhancement framework is evaluated in dynamic real-world scenarios (including cafe, street, bus, and pedestrians) at different SNR levels (ranging from low to high SNRs), using benchmark Grid and ChiME3 corpora. For objective testing, perceptual evaluation of speech quality (PESQ) is used to evaluate the quality of the restored speech. For subjective testing, the standard mean-opinion-score (MOS) method is used. Comparative experimental results show the superior performance of our proposed context-aware AV approach, over A-only, V-only, spectral subtraction (SS), and log-minimum mean square error (LMMSE) based speech enhancement methods, at both low and high SNRs. The preliminary findings demonstrate the capability of our novel approach to deal with spectro-temporal variations in real-world noisy environments, by contextually exploiting the complementary strengths of audio and visual cues. In conclusion, our contextual deep learning-driven AV framework is posited as a benchmark resource for the multi-modal speech processing and machine learning communities.
                                </p>
                                <div class="spacer-half"></div>
                                <a href="https://arxiv.org/pdf/1808.09825.pdf" class="btn-custom scroll-to">PDF</a><a href="https://doi.org/10.1016/j.inffus.2019.08.008" class="btn-custom scroll-to" style="margin-left: 10px;">DOI</a>
                            </div>
                            <div class="clearfix"></div>
                        </div>

                        <div class="inner-padding">
                            <div class="col-md-12 wow fadeIn">
                                <h3 class="mb20">CogAVHearing Lip-Reading Driven AV Speech Enhancement Demo</h3>
                                <img class="col-md-6" src="img/demos/1.png">
                                <p class="text-justify">This preliminary interactive Demo is aimed at demonstrating the potential of "context-aware" audio-visual hearing aids, based on Big Data Deep Learning technology. The demo has been developed by feeding randomly selected, real noisy speech videos from YouTube, into the audio-visual speech enhancement system.
                                </p>
                                <div class="spacer-half"></div>
                                <a href="https://cogbid.github.io/cogavhearingdemo/" class="btn-custom scroll-to">Demo Link</a>
                            </div>
                            <div class="clearfix"></div>
                        </div>
                    </div>
                </div>
            </section>
        </div>
        <!-- content close -->

        <!-- footer begin -->
        <footer>
            <div class="container">
                <div class="row">
                    <div class="col-md-6 sm-mb10">
                        <div class="mt10">&copy; Copyright 2021 - COG-MHEAR <a href="privacy-notice.html" style="padding-right: 30px;padding-left: 30px;">Privacy Notice</a> <a href="cookie-policy.html">Cookie Policy</a></div>
                    </div>
                </div>
            </div>

        </footer>
        <!-- footer close -->

        <a href="#" id="back-to-top"></a>

        <div id="preloader">
            <div class="preloader1"></div>
        </div>

    </div>

    <!-- Javascript Files
    ================================================== -->
    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/jquery.isotope.min.js"></script>
    <script src="js/easing.js"></script>
    <script src="js/owl.carousel.js"></script>
    <script src="js/jquery.countTo.js"></script>
    <script src="js/wow.min.js"></script>
    <script src="js/jquery.magnific-popup.min.js"></script>
    <script src="js/enquire.min.js"></script>
    <script src="js/jquery.stellar.min.js"></script>
    <script src="js/jquery.plugin.js"></script>
    <script src="js/jquery.easeScroll.js"></script>
    <script src="js/designesia.js"></script>
    <script src="js/validation.js"></script>

</body>

</html>
